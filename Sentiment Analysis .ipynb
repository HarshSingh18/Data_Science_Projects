{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform sentiment analysis on the movie_reviews dataset pre-loaded in the nltk.corpus library. I used tf-idf vectorizer and multinomial naive bayes algorithm to train the data and predict the sentiment on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting the Reviews(features) from the movie_reviews dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rev = movie_reviews.fileids('pos')\n",
    "len(pos_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_rev = movie_reviews.fileids('neg')\n",
    "len(neg_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying some general pre-processing steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in pos_rev:\n",
    "    rev_text_pos = nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string = \" \".join(rev_text_pos)\n",
    "    review_one_string = review_one_string.replace(' ,' , ',')\n",
    "    review_one_string = review_one_string.replace(' .' , '.')\n",
    "    review_one_string = review_one_string.replace(\"\\' \" , \"'\")\n",
    "    review_one_string = review_one_string.replace(\" \\'\", \"'\")\n",
    "    rev_list.append(review_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in neg_rev:\n",
    "    rev_text_neg = nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string = \" \".join(rev_text_neg)\n",
    "    review_one_string = review_one_string.replace(' ,' , ',')\n",
    "    review_one_string = review_one_string.replace(' .' , '.')\n",
    "    review_one_string = review_one_string.replace(\"\\' \" , \"'\")\n",
    "    review_one_string = review_one_string.replace(\" \\'\", \"'\")\n",
    "    rev_list.append(review_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       films adapted from comic books have had plenty...\n",
       "1       every now and then a movie comes along from a ...\n",
       "2       you've got mail works alot better than it dese...\n",
       "3       \" jaws \" is a rare film that grabs your attent...\n",
       "4       moviemaking is a lot like being the general ma...\n",
       "                              ...                        \n",
       "1995    if anything, \" stigmata \" should be taken as a...\n",
       "1996    john boorman's \" zardoz \" is a goofy cinematic...\n",
       "1997    the kids in the hall are an acquired taste. it...\n",
       "1998    there was a time when john carpenter was a gre...\n",
       "1999    two party guys bob their heads to haddaway's d...\n",
       "Length: 2000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(rev_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the sentiment(target) column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_targets = np.ones((1000,),dtype=np.int)\n",
    "len(pos_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_targets= np.zeros((1000,),dtype=np.int)\n",
    "len(neg_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos_tar in pos_targets:\n",
    "    target_list.append(pos_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neg_tar in neg_targets:\n",
    "    target_list.append(neg_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1995    0\n",
       "1996    0\n",
       "1997    0\n",
       "1998    0\n",
       "1999    0\n",
       "Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you've got mail works alot better than it dese...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" jaws \" is a rare film that grabs your attent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>if anything, \" stigmata \" should be taken as a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>john boorman's \" zardoz \" is a goofy cinematic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>the kids in the hall are an acquired taste. it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>two party guys bob their heads to haddaway's d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Reviews  Sentiment\n",
       "0     films adapted from comic books have had plenty...          1\n",
       "1     every now and then a movie comes along from a ...          1\n",
       "2     you've got mail works alot better than it dese...          1\n",
       "3     \" jaws \" is a rare film that grabs your attent...          1\n",
       "4     moviemaking is a lot like being the general ma...          1\n",
       "...                                                 ...        ...\n",
       "1995  if anything, \" stigmata \" should be taken as a...          0\n",
       "1996  john boorman's \" zardoz \" is a goofy cinematic...          0\n",
       "1997  the kids in the hall are an acquired taste. it...          0\n",
       "1998  there was a time when john carpenter was a gre...          0\n",
       "1999  two party guys bob their heads to haddaway's d...          0\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = pd.DataFrame(data={\"Reviews\":rev_list,\"Sentiment\":target_list})\n",
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing some more pre-processing steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       films adapted comic books plenty success, whet...\n",
       "1       every movie comes along suspect studio, every ...\n",
       "2       got mail works alot better deserves to. order ...\n",
       "3       \" jaws \" rare film grabs attention shows singl...\n",
       "4       moviemaking lot like general manager nfl team ...\n",
       "                              ...                        \n",
       "1995    anything, \" stigmata \" taken warning releasing...\n",
       "1996    john boorman's \" zardoz \" goofy cinematic deba...\n",
       "1997    kids hall acquired taste. took least season wa...\n",
       "1998    time john carpenter great horror director. cou...\n",
       "1999    two party guys bob heads haddaway's dance hit ...\n",
       "Name: Reviews, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "        no_stopword_text = [w for w in text.split() if not w in stop]\n",
    "        text = ' '.join(no_stopword_text)\n",
    "        return text\n",
    "\n",
    "Data[\"Reviews\"] = Data[\"Reviews\"].apply(lambda x: remove_stopwords(x))\n",
    "Data[\"Reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       films adapted comic books plenty success, whet...\n",
       "1       every movie comes along suspect studio, every ...\n",
       "2       got mail works alot better deserves to. order ...\n",
       "3       \" jaws \" rare film grabs attention shows singl...\n",
       "4       moviemaking lot like general manager nfl team ...\n",
       "                              ...                        \n",
       "1995    anything, \" stigmata \" taken warning releasing...\n",
       "1996    john boorman's \" zardoz \" goofy cinematic deba...\n",
       "1997    kids hall acquired taste. took least season wa...\n",
       "1998    time john carpenter great horror director. cou...\n",
       "1999    two party guys bob heads haddaway's dance hit ...\n",
       "Name: Reviews, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    text = \" \".join(lemmatized_word)\n",
    "    return text\n",
    "Data[\"Reviews\"].apply(lambda x: lemmatize(x))\n",
    "Data[\"Reviews\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying TF-IDF vectorizer on the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>10</th>\n",
       "      <th>10 000</th>\n",
       "      <th>10 10</th>\n",
       "      <th>10 minutes</th>\n",
       "      <th>10 scale</th>\n",
       "      <th>10 things</th>\n",
       "      <th>10 year</th>\n",
       "      <th>...</th>\n",
       "      <th>zest</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zeta jones</th>\n",
       "      <th>zingers</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zwick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069403</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 15926 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00      000  007   10  10 000  10 10  10 minutes  10 scale  \\\n",
       "0     0.069403  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "1     0.000000  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "2     0.000000  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "3     0.000000  0.02576  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "4     0.000000  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "...        ...      ...  ...  ...     ...    ...         ...       ...   \n",
       "1995  0.000000  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "1996  0.000000  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "1997  0.000000  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "1998  0.000000  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "1999  0.000000  0.00000  0.0  0.0     0.0    0.0         0.0       0.0   \n",
       "\n",
       "      10 things  10 year  ...  zest  zeta  zeta jones  zingers  zombie  \\\n",
       "0           0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "1           0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "2           0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "3           0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "4           0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "...         ...      ...  ...   ...   ...         ...      ...     ...   \n",
       "1995        0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "1996        0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "1997        0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "1998        0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "1999        0.0      0.0  ...   0.0   0.0         0.0      0.0     0.0   \n",
       "\n",
       "      zombies  zone  zooms  zucker  zwick  \n",
       "0         0.0   0.0    0.0     0.0    0.0  \n",
       "1         0.0   0.0    0.0     0.0    0.0  \n",
       "2         0.0   0.0    0.0     0.0    0.0  \n",
       "3         0.0   0.0    0.0     0.0    0.0  \n",
       "4         0.0   0.0    0.0     0.0    0.0  \n",
       "...       ...   ...    ...     ...    ...  \n",
       "1995      0.0   0.0    0.0     0.0    0.0  \n",
       "1996      0.0   0.0    0.0     0.0    0.0  \n",
       "1997      0.0   0.0    0.0     0.0    0.0  \n",
       "1998      0.0   0.0    0.0     0.0    0.0  \n",
       "1999      0.0   0.0    0.0     0.0    0.0  \n",
       "\n",
       "[2000 rows x 15926 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TFIDF_vect = TfidfVectorizer(ngram_range = (1,2),max_df=0.3,min_df=7)\n",
    "X = TFIDF_vect.fit_transform(Data[\"Reviews\"])\n",
    "X_names = TFIDF_vect.get_feature_names()\n",
    "X = pd.DataFrame(X.toarray(),columns=X_names)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data and then applying multinomial naive bayes algorithm on it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,Y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(Xtrain,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = NB_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200,  43],\n",
       "       [ 56, 201]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "CM = confusion_matrix(Ytest,Ypred)\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "AC = accuracy_score(Ytest,Ypred)\n",
    "AC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing hyperparameter optimization using the techniques of grid search and cross-validation in an attempt to improve the accuracy of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB2_model = MultinomialNB()\n",
    "NB2_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MultinomialNB(),\n",
       "             param_grid={'alpha': [0.2, 0.5, 0.7, 1.0, 1.2, 1.5, 1.7, 2.0, 2.5,\n",
       "                                   3.0]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid ={\"alpha\":[0.2,0.5,0.7,1.0,1.2,1.5,1.7,2.0,2.5,3.0]}\n",
    "GD_obj = GridSearchCV(NB2_model,param_grid,cv=5)\n",
    "GD_obj.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.833"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD_obj.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=2.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD_obj.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_optimized = MultinomialNB(alpha=2.0)\n",
    "NB_optimized.fit(Xtrain,Ytrain)\n",
    "Ypred_NB2 = NB_optimized.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[202,  41],\n",
       "       [ 57, 200]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_NB2 = confusion_matrix(Ytest,Ypred_NB2)\n",
    "CM_NB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.804"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AC_NB2 = accuracy_score(Ytest,Ypred_NB2)\n",
    "AC_NB2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
